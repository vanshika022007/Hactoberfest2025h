<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Feature Engineering</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/css/bootstrap.min.css" rel="stylesheet"
    integrity="sha384-T3c6CoIi6uLrA9TneNEoa7RxnatzjcDSCmG1MXxSR1GAsXEV/Dwwykc2MPK8M2HN" crossorigin="anonymous">
    <link rel="stylesheet" href="../../stylesheet.css">
    <link rel="stylesheet" type="text/css" href="https://static.javatpoint.com/link.css?v=6.0" async />
    <link rel="icon" type="image/x-icon" href="../icon/sg.png">
    <style>
        .active-submenu{
            font-weight: 500;
            background-color: darkgray;
        }
        .active-submenu:hover{
            font-weight: 500;
            background-color: darkgray;
        }
        a:hover{
            text-decoration: none;
        }
    </style>
</head>

<body style="background-image: radial-gradient(#b3d6e6, white, #b3d6e6);">
    <nav class="navbar navbar-dark bg-primary sticky-top" aria-label="Dark offcanvas navbar">
        <div class="container-fluid">
            <a class="navbar-brand" href="../../home.html">
                <img src="../icon/sg.png" alt="icon" style="height: 50px; width: 50px; margin-left: 20px;">
            </a>
            <button class="navbar-toggler" type="button" data-bs-toggle="offcanvas"
                data-bs-target="#offcanvasNavbarDark" aria-controls="offcanvasNavbarDark"
                aria-label="Toggle navigation">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="offcanvas offcanvas-end text-bg-dark" tabindex="-1" id="offcanvasNavbarDark"
                aria-labelledby="offcanvasNavbarDarkLabel">
                <div class="offcanvas-header">
                    <h5 class="offcanvas-title" id="offcanvasNavbarDarkLabel">
                        <img src="../icon/sg.png" alt="icon" style="height: 50px;">
                    </h5>
                    <button type="button" class="btn-close btn-close-white" data-bs-dismiss="offcanvas"
                        aria-label="Close"></button>
                </div>
                <div class="offcanvas-body">
                    <ul class="navbar-nav justify-content-end flex-grow-1 pe-3">
                        <li class="nav-item">
                            <a class="nav-link" aria-current="page" href="../../home.html">Home</a>
                        </li>
                        <li class="nav-item dropdown">
                            <a class="nav-link dropdown-toggle" href="#" role="button" data-bs-toggle="dropdown"
                                aria-expanded="false">
                                Programming Language
                            </a>
                            <ul class="dropdown-menu">
                                <li><a class="dropdown-item" href="../../first_page/c_page.html">C</a></li>
                                <li><a class="dropdown-item" href="../../first_page/c++_page.html">C++</a></li>
                                <li><a class="dropdown-item" href="../../first_page/java_page.html">Java</a></li>
                                <li><a class="dropdown-item" href="../../first_page/python_page.html">Python</a></li>
                            </ul>
                        </li>
                        <li class="nav-item dropdown">
                            <a class="nav-link dropdown-toggle" href="#" role="button" data-bs-toggle="dropdown"
                                aria-expanded="false">
                                Web Developement
                            </a>
                            <ul class="dropdown-menu">
                                <li><a class="dropdown-item" href="../../first_page/html_page.html">HTML</a></li>
                                <li><a class="dropdown-item" href="../../first_page/css_page.html">CSS</a></li>
                                <li><a class="dropdown-item" href="../../first_page/javascript_page.html">JavaScript</a>
                                </li>
                            </ul>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link" href="../../first_page/ml_page.html">Machine Learning</a>
                        </li>
                        <li class="nav-item dropdown">
                            <a class="nav-link active dropdown-toggle" href="#" role="button" data-bs-toggle="dropdown"
                                aria-expanded="false">
                                Machine Learning
                            </a>
                            <ul class="dropdown-menu">
                                <li><a class="dropdown-item" href="1_what_is_machine_learning.html">What is Machine Learning</a></li>
                                <li><a class="dropdown-item" href="2_application_of_ml.html">Applications of ML</a></li>
                                <li><a class="dropdown-item" href="3_life_cycle.html">Machine Learning Life Cycle</a></li>
                                <li><a class="dropdown-item" href="4_installing_anaconda_python.html">Installing Anaconda & Python</a></li>
                                <li><a class="dropdown-item" href="5_ai_vs_ml.html">Artificial Intelligence vs Machine Learning</a></li>
                                <li><a class="dropdown-item" href="6_datasets.html">Datasets</a></li>
                                <li><a class="dropdown-item" href="7_data_preprocessing.html">Data preprocessing</a></li>
                                <li><a class="dropdown-item" href="8_supervised_learning.html">Supervised Learning</a></li>
                                <li><a class="dropdown-item" href="9_unsupervised_learning.html">Unsupervised Learning</a></li>
                                <li><a class="dropdown-item" href="10_supervised_vs_unsupervised.html">Supervised Learning vs Unsupervised Learning</a></li>
                                <li><a class="dropdown-item" href="11_regression_analysis.html">Regression Analysis</a></li>
                                <li><a class="dropdown-item" href="12_linear_regression.html">Linear Regression</a></li>
                                <li><a class="dropdown-item" href="13_simple_linear_regression.html">Simple Linear Regression</a></li>
                                <li><a class="dropdown-item" href="14_multiple_linear_regression.html">Multiple Linear Regression</a></li>
                                <li><a class="dropdown-item" href="15_polynomial_regression.html">Polynomial Regression</a></li>
                                <li><a class="dropdown-item" href="16_classification_algorithm.html">Classification</a></li>
                                <li><a class="dropdown-item" href="17_logistic_regression.html">Logistic Regression</a></li>
                                <li><a class="dropdown-item" href="18_knn.html">KNN Algorithm</a></li>
                                <li><a class="dropdown-item" href="19_svm.html">Support Vector Machine</a></li>
                                <li><a class="dropdown-item" href="20_naive_bayes.html">Naive Bayes</a></li>
                                <li><a class="dropdown-item" href="21_regression_vs_classification.html">Regression vs Classification</a></li>
                                <li><a class="dropdown-item" href="22_linear_reg_vs_logistic_reg.html">Linear Regression vs Logistic Regression</a></li>
                                <li><a class="dropdown-item" href="23_decision_tree.html">Decision Tree</a></li>
                                <li><a class="dropdown-item" href="24_random_forest.html">Random Forest</a></li>
                                <li><a class="dropdown-item" href="25_clustering.html">Clustering</a></li>
                                <li><a class="dropdown-item" href="26_k_means_clustering.html">K-Means Clustering</a></li>
                                <li><a class="dropdown-item" href="27_confusion_matrix.html">Confusion Matrix</a></li>
                                <li><a class="dropdown-item" href="28_cross_validation.html">Cross Validation</a></li>
                                <li><a class="dropdown-item" href="29_dimensionality_reduction.html">Dimensionality Reduction</a></li>
                                <li><a class="dropdown-item" href="30_overfitting_underfitting.html">Overfitting & Underfitting</a></li>
                                <li><a class="dropdown-item" href="31_principle_component_analysis.html">Principle Component Analysis</a></li>
                                <li><a class="dropdown-item" href="32_p_value.html">P-Value</a></li>
                                <li><a class="dropdown-item" href="33_regularization.html">Regularization in ML</a></li>
                                <li><a class="dropdown-item" href="34_overfitting.html">Overfitting in ML</a></li>
                                <li><a class="dropdown-item" href="35_bias_variance.html">Bias & Variance</a></li>
                                <li><a class="dropdown-item" href="36_gradient_descent.html">Gradient Descent</a></li>
                                <li><a class="dropdown-item" href="37_cost_function.html">Cost Function</a></li>
                                <li><a class="dropdown-item" href="38_normalization.html">Normalization in ML</a></li>
                                <li><a class="dropdown-item" href="39_epoch_batch_iterations.html">Epoch, Batch & Iterations</a></li>
                                <li><a class="dropdown-item active-submenu" href="40_feature_engineering.html">Feature Engineering</a></li>
                                <li><a class="dropdown-item" href="41_perceptron.html">Perceptron</a></li>
                                <li><a class="dropdown-item" href="42_data_science_vs_ml.html">Data Science vs Machine Learning</a></li>
                                <li><a class="dropdown-item" href="43_ml_vs_deep_learning.html">Machine Learning vs Deep Learning</a></li>
                            </ul>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link" href="../../about.html">About Us</a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link" href="../../contact.html">Contact Us</a>
                        </li>
                    </ul>
                </div>
            </div>
        </div>
    </nav>
    <div style="padding: 30px;font-size: 18px;">
    <h1 class="h1">Feature Engineering for Machine Learning</h1>
    <p><strong>Feature engineering is the pre-processing step of machine learning, which is used to transform raw data
            into features that can be used for creating a predictive model using Machine learning or statistical
            Modelling</strong>. Feature engineering in machine learning aims to improve the performance of models. In
        this topic, we will understand the details about feature engineering in Machine learning. But before going into
        details, let's first understand what features are? And What is the need for feature engineering?</p>
    <img src="https://static.javatpoint.com/tutorial/machine-learning/images/feature-engineering-for-machine-learning.png"
        alt="Feature Engineering for Machine Learning" />
    <h2 class="h2">What is a feature?</h2>
    <p>Generally, all machine learning algorithms take input data to generate the output. The input data remains in a
        tabular form consisting of rows (instances or observations) and columns (variable or attributes), and these
        attributes are often known as <strong>features</strong>. For example, an image is an instance in computer
        vision, but a line in the image could be the feature. Similarly, in NLP, a document can be an observation, and
        the word count could be the feature. So, we can say <strong>a feature is an attribute that impacts a problem or
            is useful for the problem</strong>.</p>
    <h2 class="h2">What is Feature Engineering?</h2>
    <p><strong>Feature engineering is the pre-processing step of machine learning, which extracts features from raw
            data</strong>. It helps to represent an underlying problem to predictive models in a better way, which as a
        result, improve the accuracy of the model for unseen data. The predictive model contains predictor variables and
        an outcome variable, and while the feature engineering process selects the most useful predictor variables for
        the model.</p>
    <img src="https://static.javatpoint.com/tutorial/machine-learning/images/feature-engineering-for-machine-learning2.png"
        alt="Feature Engineering for Machine Learning" />
    <p>Since 2016, automated feature engineering is also used in different machine learning software that helps in
        automatically extracting features from raw data. Feature engineering in ML contains mainly four processes:
        <strong>Feature Creation, Transformations, Feature Extraction, and Feature Selection.</strong></p>
    <p>These processes are described as below:</p>
    <ol class="points">
        <li><strong>Feature Creation</strong>: Feature creation is finding the most useful variables to be used in a
            predictive model. The process is subjective, and it requires human creativity and intervention. The new
            features are created by mixing existing features using addition, subtraction, and ration, and these new
            features have great flexibility.</li>
        <li><strong>Transformations</strong>: The transformation step of feature engineering involves adjusting the
            predictor variable to improve the accuracy and performance of the model. For example, it ensures that the
            model is flexible to take input of the variety of data; it ensures that all the variables are on the same
            scale, making the model easier to understand. It improves the model's accuracy and ensures that all the
            features are within the acceptable range to avoid any computational error.</li>
        <li><strong>Feature Extraction</strong>: Feature extraction is an automated feature engineering process that
            generates new variables by extracting them from the raw data. The main aim of this step is to reduce the
            volume of data so that it can be easily used and managed for data modelling. Feature extraction methods
            include <strong>cluster analysis, text analytics, edge detection algorithms, and principal components
                analysis (PCA</strong>).</li>
        <li><strong>Feature Selection:</strong> While developing the machine learning model, only a few variables in the
            dataset are useful for building the model, and the rest features are either redundant or irrelevant. If we
            input the dataset with all these redundant and irrelevant features, it may negatively impact and reduce the
            overall performance and accuracy of the model. Hence it is very important to identify and select the most
            appropriate features from the data and remove the irrelevant or less important features, which is done with
            the help of feature selection in machine learning. <strong><em>"Feature selection is a way of selecting the
                    subset of the most relevant features from the original features set by removing the redundant,
                    irrelevant, or noisy features."</em></strong></li>
    </ol>
    <p>Below are some benefits of using feature selection in machine learning:</p>
    <ul class="points">
        <li>It helps in avoiding the curse of dimensionality.</li>
        <li>It helps in the simplification of the model so that the researchers can easily interpret it.</li>
        <li>It reduces the training time.</li>
        <li>It reduces overfitting hence enhancing the generalization.</li>
    </ul>
    <h2 class="h2">Need for Feature Engineering in Machine Learning</h2>
    <p>In machine learning, the performance of the model depends on data pre-processing and data handling. But if we
        create a model without pre-processing or data handling, then it may not give good accuracy. Whereas, if we apply
        feature engineering on the same model, then the accuracy of the model is enhanced. Hence, feature engineering in
        machine learning improves the model's performance. Below are some points that explain the need for feature
        engineering:</p>
    <ul class="points">
        <li><strong>Better features mean flexibility.</strong><br>
            In machine learning, we always try to choose the optimal model to get good results. However, sometimes after
            choosing the wrong model, still, we can get better predictions, and this is because of better features. The
            flexibility in features will enable you to select the less complex models. Because less complex models are
            faster to run, easier to understand and maintain, which is always desirable.</li>
        <li><strong>Better features mean simpler models.</strong><br>
            If we input the well-engineered features to our model, then even after selecting the wrong parameters (Not
            much optimal), we can have good outcomes. After feature engineering, it is not necessary to do hard for
            picking the right model with the most optimized parameters. If we have good features, we can better
            represent the complete data and use it to best characterize the given problem.</li>
        <li><strong>Better features mean better results.</strong><br>
            As already discussed, in machine learning, as data we will provide will get the same output. So, to obtain
            better results, we must need to use better features.</li>
    </ul>
    <h2 class="h2">Steps in Feature Engineering</h2>
    <p>The steps of feature engineering may vary as per different data scientists and ML engineers. However, there are
        some common steps that are involved in most machine learning algorithms, and these steps are as follows:</p>
    <ul class="points">
        <li><strong>Data Preparation:</strong> The first step is data preparation. In this step, raw data acquired from
            different resources are prepared to make it in a suitable format so that it can be used in the ML model. The
            data preparation may contain cleaning of data, delivery, data augmentation, fusion, ingestion, or loading.
        </li>
        <li><strong>Exploratory Analysis:</strong> Exploratory analysis or Exploratory data analysis (EDA) is an
            important step of features engineering, which is mainly used by data scientists. This step involves
            analysis, investing data set, and summarization of the main characteristics of data. Different data
            visualization techniques are used to better understand the manipulation of data sources, to find the most
            appropriate statistical technique for data analysis, and to select the best features for the data.</li>
        <li><strong>Benchmark</strong>: Benchmarking is a process of setting a standard baseline for accuracy to compare
            all the variables from this baseline. The benchmarking process is used to improve the predictability of the
            model and reduce the error rate.</li>
    </ul>
    <h2 class="h2">Feature Engineering Techniques</h2>
    <p>Some of the popular feature engineering techniques include:</p>
    <h3 class="h3">1. Imputation</h3>
    <p>Feature engineering deals with inappropriate data, missing values, human interruption, general errors,
        insufficient data sources, etc. Missing values within the dataset highly affect the performance of the
        algorithm, and to deal with them "Imputation" technique is used. <strong>Imputation is responsible for handling
            irregularities within the dataset.</strong> </p>
    <p>For example, removing the missing values from the complete row or complete column by a huge percentage of missing
        values. But at the same time, to maintain the data size, it is required to impute the missing data, which can be
        done as:</p>
    <ul class="points">
        <li>For numerical data imputation, a default value can be imputed in a column, and missing values can be filled
            with means or medians of the columns.</li>
        <li>For categorical data imputation, missing values can be interchanged with the maximum occurred value in a
            column.</li>
    </ul>
    <h3 class="h3">2. Handling Outliers</h3>
    <p>Outliers are the deviated values or data points that are observed too away from other data points in such a way
        that they badly affect the performance of the model. Outliers can be handled with this feature engineering
        technique. This technique first identifies the outliers and then remove them out.</p>
    <p><strong>Standard deviation</strong> can be used to identify the outliers. For example, each value within a space
        has a definite to an average distance, but if a value is greater distant than a certain value, it can be
        considered as an outlier. <strong>Z-score</strong> can also be used to detect outliers.</p>
    <h3 class="h3">3. Log transform</h3>
    <p>Logarithm transformation or log transform is one of the commonly used mathematical techniques in machine
        learning. Log transform helps in handling the skewed data, and it makes the distribution more approximate to
        normal after transformation. It also reduces the effects of outliers on the data, as because of the
        normalization of magnitude differences, a model becomes much robust.</p>
    <h4 class="n">Note: Log transformation is only applicable for the positive values; else, it will give an error. To
        avoid this, we can add 1 to the data before transformation, which ensures transformation to be positive.</h4>
    <h3 class="h3">4. Binning</h3>
    <p>In machine learning, overfitting is one of the main issues that degrade the performance of the model and which
        occurs due to a greater number of parameters and noisy data. However, one of the popular techniques of feature
        engineering, "binning", can be used to normalize the noisy data. This process involves segmenting different
        features into bins.</p>
    <h3 class="h3">5. Feature Split</h3>
    <p>As the name suggests, feature split is the process of splitting features intimately into two or more parts and
        performing to make new features. <strong>This technique helps the algorithms to better understand and learn the
            patterns in the dataset.</strong> </p>
    <p>The feature splitting process enables the new features to be clustered and binned, which results in extracting
        useful information and improving the performance of the data models.</p>
    <h3 class="h3">6. One hot encoding</h3>
    <p>One hot encoding is the popular encoding technique in machine learning. It is a technique that converts the
        categorical data in a form so that they can be easily understood by machine learning algorithms and hence can
        make a good prediction. It enables group the of categorical data without losing any information.</p>
    <h2 class="h2">Conclusion</h2>
    <p>In this topic, we have explained a detailed description of feature engineering in machine learning, working of
        feature engineering, techniques, etc.</p>
    <p>Although feature engineering helps in increasing the accuracy and performance of the model, there are also other
        methods that can increase prediction accuracy. Moreover, from the above-given techniques, there are many more
        available techniques of feature engineering, but we have mentioned the most commonly used techniques.</p>
    <hr />
</div>
<div class="d-flex justify-content-between align-items-center my-4 mx-5">
        <a href="39_epoch_batch_iterations.html"><button class="btn btn-danger" type="button">Previous</button></a>
        <a href="41_perceptron.html"><button class="btn btn-success" type="button">Next</button></a>
    </div>
    <footer>
        <div class="container-fluid bg-dark text-bg-dark"
            style="height: 50px; display: flex; align-items: center; font-size: 15px; justify-content: center;">
            Copyright&nbsp; Â©&nbsp; 2023&nbsp; StudyGenie&nbsp; -&nbsp; All Rights Reserved.
        </div>
    </footer>
</body>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/js/bootstrap.bundle.min.js"
    integrity="sha384-C6RzsynM9kWDrMNeT87bh95OGNyZPhcTNXj1NW7RuBCsyN/o0jlpcV8Qyq46cDfL"
    crossorigin="anonymous"></script>
</html>